{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T17:51:46.942696Z","iopub.execute_input":"2026-01-22T17:51:46.943004Z","iopub.status.idle":"2026-01-22T17:51:48.379029Z","shell.execute_reply.started":"2026-01-22T17:51:46.942979Z","shell.execute_reply":"2026-01-22T17:51:48.378000Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q groq qdrant-client sentence-transformers\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-22T18:09:45.415489Z","iopub.execute_input":"2026-01-22T18:09:45.415946Z","iopub.status.idle":"2026-01-22T18:09:52.635281Z","shell.execute_reply.started":"2026-01-22T18:09:45.415904Z","shell.execute_reply":"2026-01-22T18:09:52.633976Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m377.2/377.2 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport uuid\nimport re\nimport json\nimport concurrent.futures\nimport io\nimport time\nimport smtplib\nimport threading\nfrom datetime import datetime\nfrom getpass import getpass\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom groq import Groq\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nfrom sentence_transformers import SentenceTransformer\n\n# --- OPTIONAL VOICE DEPENDENCIES ---\ntry:\n    from gtts import gTTS\n    import pygame\n    try:\n        pygame.mixer.pre_init(44100, -16, 2, 2048)\n        pygame.mixer.init()\n        TTS_SUPPORT = True\n    except (pygame.error, Exception):\n        TTS_SUPPORT = False\nexcept ImportError:\n    TTS_SUPPORT = False\n\n# --- CONSTANTS ---\nSTOPWORDS = {\n    \"about\", \"after\", \"again\", \"all\", \"almost\", \"also\", \"always\", \"and\", \n    \"because\", \"before\", \"can\", \"could\", \"does\", \"down\", \"for\", \"from\", \n    \"have\", \"having\", \"here\", \"how\", \"into\", \"just\", \"know\", \"like\", \n    \"more\", \"most\", \"much\", \"next\", \"not\", \"now\", \"only\", \"out\", \"over\", \n    \"people\", \"should\", \"some\", \"still\", \"such\", \"than\", \"that\", \"the\", \n    \"then\", \"there\", \"these\", \"this\", \"time\", \"today\", \"very\", \"want\", \n    \"was\", \"what\", \"when\", \"where\", \"which\", \"who\", \"will\", \"with\", \n    \"would\", \"year\", \"your\", \"it's\", \"its\", \"user\", \"feels\" \n}\n\n# --- 1. EMAIL SYSTEM ---\nclass EmailDispatcher:\n    def __init__(self, sender_email=None, sender_password=None, recipient_email=None):\n        self.sender_email = sender_email\n        self.sender_password = sender_password\n        self.recipient_email = recipient_email\n        self.mock_mode = not (sender_email and sender_password and recipient_email)\n        \n        if self.mock_mode:\n            print(\"   [System] Email credentials missing. Running in MOCK MODE.\")\n\n    def send_email(self, subject, body_text, body_html=None):\n        print(f\"\\n   [üìß EMAIL LOG] Dispatching: '{subject}'\")\n        if self.mock_mode:\n            print(\"   \" + \"=\"*50)\n            print(f\"   [SIMULATION] To: {self.recipient_email or 'User'}\\n   [SIMULATION] Body: {body_text}\")\n            print(\"   \" + \"=\"*50)\n            return True\n\n        try:\n            msg = MIMEMultipart(\"alternative\")\n            msg[\"Subject\"] = subject\n            msg[\"From\"] = self.sender_email\n            msg[\"To\"] = self.recipient_email\n            msg.attach(MIMEText(body_text, \"plain\"))\n            if body_html:\n                msg.attach(MIMEText(body_html, \"html\"))\n\n            with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as server:\n                server.login(self.sender_email, self.sender_password)\n                server.send_message(msg)\n            return True\n        except Exception as e:\n            print(f\"   [‚ùå Error] Email failed: {e}\")\n            return False\n\n# --- 2. SENTINEL (Safety Monitor) ---\nclass Sentinel:\n    def __init__(self, bot_instance, mailer_instance):\n        self.client = bot_instance.client\n        self.memory = bot_instance.memory\n        self.mailer = mailer_instance\n        self.model = \"llama-3.1-8b-instant\"\n\n    def _analyze_trends(self, memories):\n        mem_text = \"\\n\".join([f\"- {m}\" for m in memories])\n        prompt = f\"\"\"You are 'The Sentinel', a safety monitor. Analyze these memories for triggers:\n        1. HIGH DISTRESS: Is the user spiraling?\n        2. EVENTS: Recent specific events to check on.\n        USER MEMORIES: {mem_text}\n        Output purely JSON: {{\"status\": \"ALERT/STABLE\", \"reason\": \"...\", \"action_type\": \"CHECK_IN/NONE\", \"topic\": \"...\"}}\"\"\"\n        \n        try:\n            resp = self.client.chat.completions.create(\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                model=self.model, temperature=0.1, response_format={\"type\": \"json_object\"}\n            )\n            return json.loads(resp.choices[0].message.content)\n        except:\n            return {\"status\": \"STABLE\", \"action_type\": \"NONE\"}\n\n    def _draft_notification(self, reason, topic, username):\n        prompt = f\"\"\"Write a 2-sentence warm, gentle check-in email. \n        Context: The user is feeling {reason} regarding {topic}.\n        \n        STRICT RULES:\n        1. Address the user as '{username}'.\n        2. DO NOT include introductory text like \"Here is your email\" or \"Subject:\".\n        3. DO NOT use placeholders like [Your Name] or [User Name].\n        4. Sign off as 'Vigil AI'.\n        5. Output ONLY the body of the email.\"\"\"\n        \n        try:\n            resp = self.client.chat.completions.create(\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                model=self.model, temperature=0.7\n            )\n            return resp.choices[0].message.content.strip()\n        except:\n            return f\"Hi {username}, just thinking of you and checking in. Vigil AI is here if you need to talk.\"\n\n    def _dispatch_with_delay(self, subject, llm_message, html_body):\n        \"\"\"Internal function to handle the sleep and send, intended for threading.\"\"\"\n        # Delay set to 3600 seconds (1 hour)\n        time.sleep(3600)\n        self.mailer.send_email(subject, llm_message, html_body)\n\n    def run_check(self, username):\n        try:\n            raw_data = self.memory.client.scroll(collection_name=self.memory.collection_name, limit=10)[0]\n            recent_memories = [p.payload['text'] for p in raw_data]\n            if not recent_memories: return\n\n            analysis = self._analyze_trends(recent_memories)\n            if analysis.get('status') == \"ALERT\" or analysis.get('action_type') == \"CHECK_IN\":\n                llm_message = self._draft_notification(analysis.get('reason'), analysis.get('topic', 'mood'), username)\n                subject = f\"Checking in: Regarding {analysis.get('topic', 'how you are doing')}\"\n                html_body = f\"\"\"\n                <html>\n                <body style='font-family: Arial, sans-serif; color: #333;'>\n                    <div style='max-width: 500px; padding: 20px; border: 1px solid #eee; border-radius: 10px;'>\n                        <h2 style='color: #4A90E2;'>Vigil AI Care Team</h2>\n                        <p style='font-size: 16px; line-height: 1.5;'>{llm_message}</p>\n                    </div>\n                </body>\n                </html>\n                \"\"\"\n                # Start delayed dispatch in a separate thread to keep chat responsive\n                threading.Thread(\n                    target=self._dispatch_with_delay, \n                    args=(subject, llm_message, html_body),\n                    daemon=True\n                ).start()\n        except Exception as e:\n            print(f\"   [Sentinel Error] {e}\")\n\n# --- 3. MEMORY MANAGER ---\nclass MemoryManager:\n    def __init__(self):\n        print(\"   [System] Initializing Memory Core...\")\n        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n        self.client = QdrantClient(location=\":memory:\")\n        self.collection_name = \"therapy_memories\"\n        self.client.recreate_collection(\n            collection_name=self.collection_name,\n            vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE)\n        )\n\n    def _tokenize(self, text):\n        clean_text = re.sub(r'[^\\w\\s]', '', text.lower())\n        words = {w for w in clean_text.split() if w not in STOPWORDS and len(w) > 3}\n        return words\n\n    def add_memory(self, text):\n        # Removed the console logging for database writes as requested\n        vector = self.encoder.encode(text).tolist()\n        payload = {\"text\": text, \"timestamp\": datetime.now().isoformat()}\n        self.client.upsert(\n            collection_name=self.collection_name,\n            points=[models.PointStruct(id=str(uuid.uuid4()), vector=vector, payload=payload)]\n        )\n\n    def search_memories(self, query_text, limit=3, score_threshold=0.35):\n        query_vector = self.encoder.encode(query_text).tolist()\n        hits = []\n        found_texts = set()\n        try:\n            results = self.client.search(collection_name=self.collection_name, query_vector=query_vector, limit=limit, score_threshold=score_threshold)\n            for hit in results:\n                hits.append(hit.payload['text'])\n                found_texts.add(hit.payload['text'])\n        except Exception: pass\n        if len(hits) < limit:\n            try:\n                query_keywords = self._tokenize(query_text)\n                if query_keywords:\n                    all_points = self.client.scroll(collection_name=self.collection_name, limit=100)[0]\n                    for point in all_points:\n                        text = point.payload['text']\n                        if text in found_texts: continue\n                        if query_keywords.intersection(self._tokenize(text)):\n                            hits.append(text); found_texts.add(text)\n                            if len(hits) >= limit: break\n            except: pass\n        return hits[:limit]\n\n# --- 4. STT & TTS AGENTS ---\nclass STTAgent:\n    def __init__(self, client): self.client = client\n    def transcribe_file(self, file_path):\n        if not os.path.exists(file_path): return f\"Error: File '{file_path}' not found.\"\n        try:\n            with open(file_path, \"rb\") as file:\n                transcription = self.client.audio.transcriptions.create(file=(os.path.basename(file_path), file), model=\"whisper-large-v3\", response_format=\"text\")\n            return str(transcription).strip()\n        except Exception as e: return f\"STT Agent Error: {e}\"\n\nclass TTSAgent:\n    def __init__(self):\n        try:\n            from gtts import gTTS\n            self.gtts_available = True\n        except ImportError: self.gtts_available = False\n        self.can_play = TTS_SUPPORT\n    def speak(self, text):\n        if not self.gtts_available: return\n        try:\n            tts = gTTS(text=text, lang='en')\n            temp_file = \"last_reply.mp3\"\n            tts.save(temp_file)\n            if self.can_play:\n                pygame.mixer.music.load(temp_file); pygame.mixer.music.play()\n                while pygame.mixer.music.get_busy(): time.sleep(0.1)\n                pygame.mixer.music.unload()\n        except: pass\n\n# --- 5. SPECIALIST AGENTS ---\nclass SpecialistAgents:\n    def __init__(self, client):\n        self.client = client\n        self.model = \"llama-3.1-8b-instant\"\n    def _call_llm(self, sys, usr, temp=0.6):\n        try:\n            resp = self.client.chat.completions.create(messages=[{\"role\": \"system\", \"content\": sys}, {\"role\": \"user\", \"content\": usr}], model=self.model, temperature=temp, max_tokens=1000)\n            return resp.choices[0].message.content\n        except Exception as e: return f\"Error: {e}\"\n    def run_validator(self, text):\n        return self._call_llm(\"You are 'The Validator'. Focus on empathy and warmth. Keep it under 2 sentences.\", text)\n    def run_challenger(self, text):\n        return self._call_llm(\"You are 'The Challenger'. Identify CBT distortions. Be analytical. Keep it under 2 sentences.\", text)\n    def run_summarizer(self, text):\n        prompt = \"\"\"Summarize this text into a concise memory key. Output ONLY the memory key string. STRICTLY FORBIDDEN: Do not include \"I'm sorry\", \"Here is your key\", \"User feels\", or conversational filler. PRESERVE proper nouns and specific events. Format: 'Sadness about [specific event] on [date/time]'.\"\"\"\n        return self._call_llm(prompt, text, temp=0.1)\n    def run_synthesizer(self, original, validation, challenge, history, has_history, state_instr):\n        prompt = f\"You are 'Vigil' (formerly Anamnesis), a Lead Therapist.\\n\\n### TEAM REPORTS:\\n1. VALIDATOR: {validation}\\n2. CHALLENGER: {challenge}\\n3. HISTORIAN: {history}\\n\\n### DYNAMIC INSTRUCTIONS:\\n{state_instr}\\n\\n### FINAL GOAL:\\nCraft a natural, seamless response. No jargon. No JSON. Reference history if {has_history}.\"\n        return self._call_llm(prompt, original)\n\n# --- 6. COUNCIL ORCHESTRATOR ---\nclass CouncilOrchestrator:\n    def __init__(self):\n        # Hardcoded Groq API Key as requested\n        self.api_key = \"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\"\n        os.environ[\"GROQ_API_KEY\"] = self.api_key\n        self.client = Groq(api_key=self.api_key)\n        self.memory = MemoryManager()\n        self.stt_agent = STTAgent(self.client); self.tts_agent = TTSAgent(); self.agents = SpecialistAgents(self.client)\n        self.turn_count = 0; self.last_input_length = 0\n    def _is_prime(self, n):\n        if n < 2: return False\n        for i in range(2, int(n**0.5) + 1):\n            if n % i == 0: return False\n        return True\n    def consult(self, user_input):\n        self.turn_count += 1\n        words = user_input.split(); input_len = len(words)\n        state_instr = \"\"\n        if input_len > 25:\n            q = \"Only ask a question if turn count is a prime number.\" if self._is_prime(self.turn_count) else \"Stay in quiet active listening mode.\"\n            state_instr = f\"USER IS VENTING. {q} Focus on validation.\"\n        elif input_len < 6 and self.last_input_length > 20: state_instr = \"USER IS COLLAPSING. Step in now: Summarize their session, use a brief analogy, and provide a path forward.\"\n        elif self.turn_count < 3: state_instr = \"PROBING PHASE. Be brief. Ask one gentle question.\"\n        else: state_instr = \"Normal dialogue. Maintain flow. Use Challenger logic gently.\"\n        self.last_input_length = input_len\n        with concurrent.futures.ThreadPoolExecutor() as exec:\n            f_val = exec.submit(self.agents.run_validator, user_input)\n            f_chal = exec.submit(self.agents.run_challenger, user_input)\n            f_hist = exec.submit(self.memory.search_memories, user_input)\n            f_mem = exec.submit(self.agents.run_summarizer, user_input)\n            v, c, h, m = f_val.result(), f_chal.result(), f_hist.result(), f_mem.result()\n        history_text = \"\\n\".join([f\"- {i}\" for i in h]) if h else \"No context.\"\n        final = self.agents.run_synthesizer(user_input, v, c, history_text, len(h)>0, state_instr)\n        self.memory.add_memory(m)\n        return final\n\n# --- 7. CONTINUOUS CHAT ---\ndef start_chat():\n    print(\"\\n\" + \"=\"*60 + \"\\n      VIGIL AI: THE SENTINEL COUNCIL\\n\" + \"=\"*60 + \"\\n\")\n    \n    # Persistent User Logic\n    username = input(\"Username: \").strip().lower()\n    \n    # Path for Kaggle or Local Environment\n    user_db_file = \"vigil_users.json\"\n    if os.path.exists(\"/kaggle/working\"):\n        user_db_file = os.path.join(\"/kaggle/working\", user_db_file)\n    \n    users = {}\n    if os.path.exists(user_db_file):\n        try:\n            with open(user_db_file, 'r') as f:\n                users = json.load(f)\n        except:\n            users = {}\n    \n    email = users.get(username)\n    if not email:\n        email = input(f\"New user '{username}' detected. Please provide your email for safety alerts: \").strip()\n        users[username] = email\n        with open(user_db_file, 'w') as f:\n            json.dump(users, f)\n    else:\n        print(f\"Welcome back, {username} ({email})\")\n\n    bot = CouncilOrchestrator()\n    mailer = EmailDispatcher(\"ishansingh.iwg@gmail.com\", \"fkpy jpoe bkxv dvyb\", email)\n    sentinel = Sentinel(bot, mailer)\n    voice_enabled = bot.tts_agent.gtts_available \n\n    print(\"\\nVIGIL: I'm here. Whatever you're carrying, we can look at it together.\\n\")\n\n    while True:\n        try:\n            user_input = input(\"YOU: \").strip()\n            if user_input.lower() in ['exit', 'quit']: break\n            if user_input.lower() == 'mute':\n                voice_enabled = not voice_enabled\n                print(f\"   [System] Voice: {'ON' if voice_enabled else 'OFF'}\")\n                continue\n            if user_input.lower() == 'v':\n                path = input(\"Audio path: \").strip()\n                user_input = bot.stt_agent.transcribe_file(path)\n                if \"Error\" in user_input: print(user_input); continue\n                print(f\"(Transcribed): {user_input}\")\n            if not user_input: continue\n\n            response = bot.consult(user_input)\n            print(f\"\\nVIGIL: {response}\\n\")\n            if voice_enabled: bot.tts_agent.speak(response)\n\n            if bot.turn_count % 3 == 0:\n                sentinel.run_check(username)\n                \n        except KeyboardInterrupt: break\n\nif __name__ == \"__main__\":\n    start_chat()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T18:10:08.656446Z","iopub.execute_input":"2026-01-22T18:10:08.656819Z"}},"outputs":[{"name":"stderr","text":"2026-01-22 18:10:29.409129: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769105429.649015      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769105429.725317      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769105430.282818      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769105430.282862      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769105430.282865      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769105430.282868      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n      VIGIL AI: THE SENTINEL COUNCIL\n============================================================\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Username:  daksh\nNew user 'daksh' detected. Please provide your email for safety alerts:  dadeech@mgmail.com\n"},{"name":"stdout","text":"   [System] Initializing Memory Core...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a93085cec81a4c739839a2739a0965e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2fc41e02ebe4b1cbf21306bd6fbd6e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51418c06c44246a9ad116c8bf5a5855f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63b5a83ccfe43e5b1954fc24f01d78c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7c3c6b86567475094106d3b0ba415ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0d6e6958ed54a21835dde9eacc63b22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0fb534b9a8142d3bdf46acd1458f6d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed9446e25e440b5970634dfa9db87f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d96c0463812341e28b634368585cfd33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4787e6451c174e71a8f013ac9ec440fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d659465dd9846f0b6eecdf7e71f3d74"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_55/1058259498.py:167: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n  self.client.recreate_collection(\n","output_type":"stream"},{"name":"stdout","text":"\nVIGIL: I'm here. Whatever you're carrying, we can look at it together.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"YOU:  im not feeling well at all\n"},{"name":"stdout","text":"\nVIGIL: I'm so sorry to hear that you're not feeling well. Can you tell me a bit more about what's going on - is it physical or emotional, or a bit of both?\n\n","output_type":"stream"}],"execution_count":null}]}